#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "matplotlib",
# ]
# ///
"""
Analyze DWARF debug stats JSON files and produce aggregated statistics.

Usage:
    uv run analyze_debug_stats.py [OPTIONS] [DIRECTORY]

This script searches for all *.debug-stats.json files recursively from the specified
directory (or current directory if not provided) and generates a comprehensive markdown
report of DWARF debugging statistics.

Arguments:
    DIRECTORY              Directory to search for *.debug-stats.json files (default: current directory)

Options:
    --measure FILE         Measure the size of FILE and include in summary. Can be specified multiple times.
                          Also checks for FILE.debug and includes its size if it exists.

Prerequisites:
    - uv package manager
    - Python 3.10 or later (handled by uv)
    - matplotlib library (automatically installed by uv from script dependencies)

Input:
    - Searches for all *.debug-stats.json files in the specified directory and subdirectories
    - Files should be generated by compiling with -ddwarf-metrics flag

Output:
    - Markdown-formatted report printed to stdout
    - Includes histograms (as inline SVG), top values analysis, and file-level statistics
    - Summary includes total files processed, variables analyzed, and DWARF DIEs generated

Example workflow:
    1. Compile OCaml files with debugging flags:
       ocamlopt -ddwarf-metrics -gno-upstream-dwarf -shape-format debugging-shapes -g file.ml

    2. Run analysis:
       uv run analyze_debug_stats.py . > report.md

    3. View the generated markdown report in a markdown renderer:
       mdcat report.md
       # or open in your preferred markdown viewer/browser
"""

import json
import glob
import os
import sys
import argparse
import subprocess
import re
from collections import defaultdict

import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
from io import BytesIO

class StatEntry:
    def __init__(self, type_name, shape_size_before_reduction_in_bytes, shape_size_after_reduction_in_bytes, shape_size_after_evaluation_in_bytes,
                 reduction_steps,
                 evaluation_steps, dwarf_die_size, cms_files_loaded, cms_files_cached,
                 cms_files_missing, cms_files_unreadable, file_path):
        self.type_name = type_name
        self.shape_size_before_reduction_in_bytes = shape_size_before_reduction_in_bytes
        self.shape_size_after_reduction_in_bytes = shape_size_after_reduction_in_bytes
        self.shape_size_after_evaluation_in_bytes = shape_size_after_evaluation_in_bytes
        self.reduction_steps = reduction_steps
        self.evaluation_steps = evaluation_steps
        self.dwarf_die_size = dwarf_die_size
        self.cms_files_loaded = cms_files_loaded
        self.cms_files_cached = cms_files_cached
        self.cms_files_missing = cms_files_missing
        self.cms_files_unreadable = cms_files_unreadable
        self.file_path = file_path

class DwarfSection:
    def __init__(self, name, compressed_size, uncompressed_size, is_compressed):
        self.name = name
        self.compressed_size = compressed_size
        self.uncompressed_size = uncompressed_size
        self.is_compressed = is_compressed

class FileMeasurement:
    def __init__(self, file_path, main_size, debug_size=None, debug_uncompressed_size=None,
                 dwarf_sections=[], total_dwarf_compressed=0, total_dwarf_uncompressed=0):
        self.file_path = file_path
        self.main_size = main_size
        self.debug_size = debug_size
        self.debug_uncompressed_size = debug_uncompressed_size
        self.dwarf_sections = dwarf_sections
        self.total_dwarf_compressed = total_dwarf_compressed
        self.total_dwarf_uncompressed = total_dwarf_uncompressed

def parse_json_file(file_path):
    """Parse a single JSON file and return (entries, file_metadata) tuple."""
    entries = []
    file_metadata = {'compilation_parameters': None}
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)

        # Extract compilation parameters
        file_metadata['compilation_parameters'] = data.get('compilation_parameters', None)

        # Process variables
        for var_data in data.get('variables', []):
            try:
                entry = StatEntry(
                    type_name=var_data['type'],
                    shape_size_before_reduction_in_bytes=int(var_data['shape_size_before_reduction_in_bytes']),
                    shape_size_after_reduction_in_bytes=int(var_data['shape_size_after_reduction_in_bytes']),
                    shape_size_after_evaluation_in_bytes=int(var_data['shape_size_after_evaluation_in_bytes']),
                    reduction_steps=int(var_data['reduction_steps']),
                    evaluation_steps=int(var_data['evaluation_steps']),
                    dwarf_die_size=int(var_data['dwarf_die_size']),
                    cms_files_loaded=int(var_data['cms_files_loaded']),
                    cms_files_cached=int(var_data['cms_files_cached']),
                    cms_files_missing=var_data.get('cms_files_missing', []),
                    cms_files_unreadable=var_data.get('cms_files_unreadable', []),
                    file_path=file_path
                )
                entries.append(entry)
            except (ValueError, KeyError) as e:
                print("Warning: Skipping malformed variable in {}: {}".format(file_path, e))

    except (json.JSONDecodeError, IOError) as e:
        print("Warning: Could not parse {}: {}".format(file_path, e))

    return entries, file_metadata



# CR sspies: The analysis of the DWARF sections below currently only works on
# Linux, but not yet on macOS. It searches for Linux section names, and it uses
# tools not always available on macOS. Add macOS support in the future. Note
# that this requires handling the .dSYM files produced on macOS by dsymutil
# instead of .debug files. See #4531.
def analyze_dwarf_sections(file_path):
    """Analyze DWARF sections in a binary using readelf and objdump, detecting compression."""
    try:
        # Use readelf to determine the sizes of the sections (compressed)
        readelf_result = subprocess.run(['readelf', '-SW', file_path],
                                      capture_output=True, text=True, check=True)

        section_pattern = re.compile(r'^\s*\[\s*\d+\]\s+(\.(?:z)?debug\w*)\s+\w+\s+[0-9a-fA-F]+\s+[0-9a-fA-F]+\s+([0-9a-fA-F]+)\s+[0-9a-fA-F]+\s+(.*)$')

        # Use objdump to determine the sizes of the sections (uncompressed)
        objdump_result = subprocess.run(['objdump', '-h', file_path],
                                      capture_output=True, text=True, check=True)

        dwarf_sections = {}

        # First pass: Parse readelf output to get compression info and compressed sizes
        for line in readelf_result.stdout.split('\n'):
            match = section_pattern.match(line)
            if match:
                section_name = match.group(1)
                size_hex = match.group(2)
                flags = match.group(3)

                try:
                    compressed_size = int(size_hex, 16)
                    is_compressed = (section_name.startswith('.zdebug') or 'C' in flags)

                    # Normalize section name (remove 'z' prefix if present)
                    normalized_name = section_name.replace('.zdebug', '.debug')

                    dwarf_sections[normalized_name] = DwarfSection(
                        normalized_name, compressed_size, compressed_size, is_compressed
                    )
                except ValueError:
                    continue

        # Second pass: Parse objdump output to get uncompressed sizes
        for line in objdump_result.stdout.split('\n'):
            # objdump format: Idx Name Size VMA LMA File off Algn
            if '.debug' in line:
                parts = line.split()
                if len(parts) >= 3:
                    section_name = parts[1]  # Name column
                    size_hex = parts[2]      # Size column (uncompressed)

                    try:
                        uncompressed_size = int(size_hex, 16)
                        if section_name in dwarf_sections:
                            dwarf_sections[section_name].uncompressed_size = uncompressed_size
                        else:
                            dwarf_sections[section_name] = DwarfSection(
                                section_name, uncompressed_size, uncompressed_size, False
                            )
                    except (ValueError, IndexError):
                        continue

        return dwarf_sections
    except (subprocess.CalledProcessError, FileNotFoundError):
        return {}

def measure_file_sizes(file_paths):
    """Measure the sizes of the specified files and their .debug variants if they exist."""
    measurements = []

    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File '{file_path}' does not exist, skipping measurement.", file=sys.stderr)
            continue

        # Get the main file size
        main_size = os.path.getsize(file_path)

        # Check for .debug variant
        debug_file_path = file_path + ".debug"
        debug_size = None
        if os.path.exists(debug_file_path):
            debug_size = os.path.getsize(debug_file_path)

        # Analyze DWARF sections in the main binary
        dwarf_sections = analyze_dwarf_sections(file_path)

        # Also analyze DWARF sections in the .debug file if it exists
        debug_uncompressed_size = None
        if debug_size is not None:
            debug_dwarf_sections = analyze_dwarf_sections(debug_file_path)
            # Calculate uncompressed size of debug sidecar from its DWARF sections
            debug_uncompressed_size = sum(section.uncompressed_size for section in debug_dwarf_sections.values())

            # Merge the sections, summing sizes for sections that appear in both
            for section_name, section_data in debug_dwarf_sections.items():
                if section_name in dwarf_sections:
                    # Sum both compressed and uncompressed sizes
                    dwarf_sections[section_name].compressed_size += section_data.compressed_size
                    dwarf_sections[section_name].uncompressed_size += section_data.uncompressed_size
                    # Keep compression status (any compression counts as compressed)
                    dwarf_sections[section_name].is_compressed = (
                        dwarf_sections[section_name].is_compressed or section_data.is_compressed
                    )
                else:
                    dwarf_sections[section_name] = section_data

        # Calculate totals
        total_dwarf_compressed = sum(section.compressed_size for section in dwarf_sections.values())
        total_dwarf_uncompressed = sum(section.uncompressed_size for section in dwarf_sections.values())

        measurements.append(FileMeasurement(file_path, main_size, debug_size, debug_uncompressed_size,
                                          list(dwarf_sections.values()), total_dwarf_compressed, total_dwarf_uncompressed))

    return measurements

# Not currently used - kept for potential fallback
def create_histogram_text(values, title, num_buckets=10):
    """Create a text-based histogram with integer buckets."""
    if not values:
        return "{}: No data\n".format(title)

    min_val = min(values)
    max_val = max(values)

    if min_val == max_val:
        return "{}: All values are {}\n".format(title, min_val)

    # Calculate integer bucket size, ensuring we cover the full range
    range_size = max_val - min_val
    bucket_size = max(1, (range_size + num_buckets - 1) // num_buckets)  # Round up division

    buckets = [0] * num_buckets
    bucket_ranges = []

    for i in range(num_buckets):
        start = min_val + i * bucket_size
        end = min_val + (i + 1) * bucket_size
        bucket_ranges.append((start, end))

    # Assign values to buckets
    for val in values:
        bucket_idx = min((val - min_val) // bucket_size, num_buckets - 1)
        buckets[bucket_idx] += 1

    # Create text histogram
    result = "{} (Range: {}-{}):\n".format(title, min_val, max_val)
    max_count = max(buckets)
    scale = 50.0 / max_count if max_count > 0 else 1

    for i, count in enumerate(buckets):
        start, end = bucket_ranges[i]
        bar_length = int(count * scale)
        bar = '#' * bar_length
        # Show inclusive start, exclusive end for integer ranges with comma formatting
        result += "[{:>8,}-{:>8,}): {:>6,} {}\n".format(start, end, count, bar)

    return result + "\n"

def plot_histogram_on_axis(ax, values, bins, title, xlabel='Value', ylabel='Frequency', log_scale=True):
    """Helper function to plot a histogram on a given axis."""
    ax.hist(values, bins=bins, edgecolor='black', alpha=0.7)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.grid(True, alpha=0.3)
    ax.ticklabel_format(style='plain')
    if log_scale:
        ax.set_yscale('log')

def create_dual_histograms(values, title, num_buckets=10, xlabel='Value'):
    """Create side-by-side log scale and focused histograms and return as SVG."""
    min_val = min(values)
    max_val = max(values)

    # Calculate focused histogram data
    range_size = max_val - min_val
    bucket_size = max(1, (range_size + num_buckets - 1) // num_buckets)

    buckets = [0] * num_buckets
    bucket_ranges = []

    for i in range(num_buckets):
        start = min_val + i * bucket_size
        end = min_val + (i + 1) * bucket_size
        bucket_ranges.append((start, end))

    # Assign values to buckets
    for val in values:
        bucket_idx = min((val - min_val) // bucket_size, num_buckets - 1)
        buckets[bucket_idx] += 1

    # Find the bucket with the most entries
    max_bucket_idx = buckets.index(max(buckets))
    focus_start, focus_end = bucket_ranges[max_bucket_idx]

    # Filter values to only those in the largest bucket
    focused_values = [v for v in values if focus_start <= v < focus_end]

    # Always create side-by-side subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))

    # Log scale histogram
    plot_histogram_on_axis(ax1, values, num_buckets, f"{title} (Log Scale)",
                         xlabel=f"{xlabel} ({min_val:,}-{max_val:,})")

    # Focused histogram (even if it doesn't make much sense)
    focused_bins = min(10, len(set(focused_values))) if len(focused_values) > 1 else 1
    plot_histogram_on_axis(ax2, focused_values, focused_bins,
                         f"{title} (Focused Log Scale)",
                         xlabel=f"{xlabel} ({focus_start:,}-{focus_end:,})")

    # Save to SVG in memory
    buf = BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='svg', bbox_inches='tight')
    plt.close()

    # Get SVG data and remove newlines/whitespace
    buf.seek(0)
    svg_data = buf.getvalue().decode('utf-8')
    buf.close()

    # Remove newlines and excess whitespace to make it a single line
    svg_data = ' '.join(svg_data.split())

    return svg_data

def create_histogram(values, title, num_buckets=10, xlabel='Value'):
    """Create histogram using matplotlib."""
    if not values:
        return f"{title}: No data\n\n"

    svg_data = create_dual_histograms(values, title, num_buckets, xlabel=xlabel)
    return f"\n{svg_data}\n"

def display_file_measurements(file_measurements):
    """Display file size measurements if any are provided."""
    if file_measurements:
        print("# File Measurements")
        print()
        for measurement in file_measurements:
            # Total size is main binary + debug sidecar (they are separate files)
            total_size = measurement.main_size + (measurement.debug_size or 0)

            # Calculate percentages based on actual on-disk sizes (compressed)
            dwarf_compressed_percentage = (measurement.total_dwarf_compressed / total_size * 100) if total_size > 0 else 0

            print("## `{}`".format(measurement.file_path))
            print("- **Total file size**: {:,} bytes".format(total_size))
            if measurement.debug_size is not None:
                print("  - Main binary: {:,} bytes".format(measurement.main_size))
                print("  - Debug sidecar: {:,} bytes".format(measurement.debug_size))

            # Show DWARF section info with compression details in single line
            if measurement.total_dwarf_compressed != measurement.total_dwarf_uncompressed:
                compression_ratio = measurement.total_dwarf_uncompressed / measurement.total_dwarf_compressed if measurement.total_dwarf_compressed > 0 else 1
                print("- **DWARF sections**: {:,} bytes ({:,} bytes uncompressed; {:.1f}x); {:.1f}% of total".format(
                    measurement.total_dwarf_compressed, measurement.total_dwarf_uncompressed, compression_ratio,
                    dwarf_compressed_percentage))
            else:
                print("- **DWARF sections**: {:,} bytes ({:.1f}% of total)".format(
                    measurement.total_dwarf_compressed, dwarf_compressed_percentage))

            if measurement.dwarf_sections:
                # Sort sections by compressed size descending
                sorted_sections = sorted(measurement.dwarf_sections,
                                       key=lambda x: x.compressed_size, reverse=True)

                for section in sorted_sections:
                    # Calculate percentage of total DWARF compressed size
                    section_percentage = (section.compressed_size / measurement.total_dwarf_compressed * 100) if measurement.total_dwarf_compressed > 0 else 0

                    if section.is_compressed and section.compressed_size != section.uncompressed_size:
                        section_ratio = section.uncompressed_size / section.compressed_size if section.compressed_size > 0 else 1
                        print("  - `{}`: {:,} bytes compressed ({:,} bytes uncompressed; {:.1f}x ratio), {:.1f}% of compressed DWARF".format(
                            section.name, section.compressed_size, section.uncompressed_size, section_ratio, section_percentage))
                    else:
                        print("  - `{}`: {:,} bytes (not compressed); {:.1f}% of DWARF".format(
                            section.name, section.compressed_size, section_percentage))
            print()

def analyze_stats(search_dir="."):
    """Main analysis function."""
    # Find all JSON files in the specified directory
    search_pattern = os.path.join(search_dir, "**/*.debug-stats.json")
    json_files = glob.glob(search_pattern, recursive=True)

    if not json_files:
        print(f"No .debug-stats.json files found in '{search_dir}' or its subdirectories.")
        return

    # Parse all files
    all_entries = []
    all_file_metadata = []
    all_missing_files = set()
    all_unreadable_files = set()
    file_count = 0
    for json_file in json_files:
        entries, file_metadata = parse_json_file(json_file)
        if entries:
            all_entries.extend(entries)
            all_file_metadata.append((json_file, file_metadata))
            file_count += 1
            # Collect missing/unreadable files from all variables in this file
            for entry in entries:
                all_missing_files.update(entry.cms_files_missing)
                all_unreadable_files.update(entry.cms_files_unreadable)

    if not all_entries:
        print("No valid entries found.")
        return

    # Calculate total DWARF DIEs
    total_dwarf_dies = sum(e.dwarf_die_size for e in all_entries)

    # CMS file statistics for summary - aggregate from per-variable data
    total_cms_loaded = sum(e.cms_files_loaded for e in all_entries)
    total_cms_cached = sum(e.cms_files_cached for e in all_entries)

    # Count files that have variables with CMS files loaded
    files_with_cms = set()
    for entry in all_entries:
        if entry.cms_files_loaded > 0:
            files_with_cms.add(entry.file_path)
    files_with_cms_loaded = len(files_with_cms)

    # Print summary header
    print("# DWARF Debug Statistics Analysis")
    print()
    print("- **Summary:** {:,} files, {:,} variables, {:,} total DIEs".format(file_count, len(all_entries), total_dwarf_dies))
    print("- **CMS Files:** {:,} loaded across {:,} files, {:,} cache hits".format(total_cms_loaded, files_with_cms_loaded, total_cms_cached))

    # Print compilation parameters - fail if not consistent
    compilation_params_sets = set()
    for _, metadata in all_file_metadata:
        params = metadata.get('compilation_parameters')
        if params:
            # Convert to a frozenset of items for hashability
            params_tuple = tuple(sorted(params.items()))
            compilation_params_sets.add(params_tuple)

    if not compilation_params_sets:
        print("- **Error:** No compilation parameters found in JSON files.")
    elif len(compilation_params_sets) > 1:
        print("- **Error:** Files were compiled with different parameters. All files must use the same configuration.")
    else:
        # All files used the same parameters
        params_dict = dict(list(compilation_params_sets)[0])
        config_parts = []
        for key, value in sorted(params_dict.items()):
            # Shorten parameter names for display
            short_name = key.replace('gdwarf_config_', '').replace('_', '-')
            config_parts.append("{}={}".format(short_name, value))
        print("- **Configuration:** {}".format(", ".join(config_parts)))
    print()

    print("## Individual Variable Statistics")
    print()


    # Aggregate statistics for individual variables
    shape_sizes_before_reduction = [e.shape_size_before_reduction_in_bytes for e in all_entries]
    shape_sizes_after_reduction = [e.shape_size_after_reduction_in_bytes for e in all_entries]
    shape_sizes_after_evaluation = [e.shape_size_after_evaluation_in_bytes for e in all_entries]
    reduction_steps = [e.reduction_steps for e in all_entries]
    evaluation_steps = [e.evaluation_steps for e in all_entries]
    dwarf_die_sizes = [e.dwarf_die_size for e in all_entries]
    cms_files_loaded = [e.cms_files_loaded for e in all_entries]
    cms_files_cached = [e.cms_files_cached for e in all_entries]

    types = [e.type_name for e in all_entries]

    # Top values analysis function
    def print_top_values(values, labels, title, n=5, unit=""):
        """Print top N values with their labels, deduplicated by (value, label) pairs."""
        if not values:
            return

        # Create a set to deduplicate (value, label) pairs
        unique_pairs = set(zip(values, labels))

        # Sort by value descending
        sorted_pairs = sorted(unique_pairs, key=lambda x: x[0], reverse=True)

        print("#### {}".format(title))
        print()
        for i, (value, label) in enumerate(sorted_pairs[:n]):
            if unit:
                print("{:2d}. **{:,} {}** - `{}`".format(i+1, value, unit, label))
            else:
                print("{:2d}. **{:,}** - `{}`".format(i+1, value, label))
        print()

    print("### Shape Sizes Before Reduction")
    print(create_histogram(shape_sizes_before_reduction, "Shape Sizes Before Reduction", xlabel="Size (bytes)"), end="")
    print_top_values(shape_sizes_before_reduction, types, "Top 5 Shape Sizes Before Reduction", unit="bytes")

    # Shape Sizes After Reduction section
    print("### Shape Sizes After Reduction")
    print(create_histogram(shape_sizes_after_reduction, "Shape Sizes After Reduction", xlabel="Size (bytes)"), end="")
    print_top_values(shape_sizes_after_reduction, types, "Top 5 Shape Sizes After Reduction", unit="bytes")

    print("### Shape Sizes After Evaluation")
    print(create_histogram(shape_sizes_after_evaluation, "Shape Sizes After Evaluation", xlabel="Size (bytes)"), end="")
    print_top_values(shape_sizes_after_evaluation, types, "Top 5 Shape Sizes After Evaluation", unit="bytes")

    # Reduction Steps section
    print("### Reduction Steps")
    print(create_histogram(reduction_steps, "Reduction Steps", xlabel="Steps"), end="")
    print_top_values(reduction_steps, types, "Top 5 Reduction Steps", unit="steps")

    # Evaluation Steps section
    print("### Evaluation Steps")
    print(create_histogram(evaluation_steps, "Evaluation Steps", xlabel="Steps"), end="")
    print_top_values(evaluation_steps, types, "Top 5 Evaluation Steps", unit="steps")

    # DWARF Sizes section
    print("### DWARF Sizes")
    print(create_histogram(dwarf_die_sizes, "DWARF Sizes", xlabel="DWARF DIEs"), end="")
    print_top_values(dwarf_die_sizes, types, "Top 5 DWARF Sizes", unit="DWARF DIEs")

    # CMS Files Loaded section
    print("### CMS Files Loaded")
    print(create_histogram(cms_files_loaded, "CMS Files Loaded", xlabel="Files"), end="")
    print_top_values(cms_files_loaded, types, "Top 5 CMS Files Loaded", unit="files")

    # CMS Cache Hits section
    print("### CMS Cache Hits")
    print(create_histogram(cms_files_cached, "CMS Cache Hits", xlabel="Cache hits"), end="")
    print_top_values(cms_files_cached, types, "Top 5 CMS Cache Hits", unit="cache hits")

    # File-level aggregation and statistics
    file_stats = defaultdict(lambda: {'count': 0, 'total_shape_size_before_reduction': 0, 'total_shape_size_after_reduction': 0, 'total_shape_size_after_evaluation': 0, 'total_dwarf_dies': 0, 'cms_files_loaded': 0, 'cms_files_cached': 0})

    for entry in all_entries:
        basename = os.path.basename(entry.file_path)
        file_stats[basename]['count'] += 1
        file_stats[basename]['total_shape_size_before_reduction'] += entry.shape_size_before_reduction_in_bytes
        file_stats[basename]['total_shape_size_after_reduction'] += entry.shape_size_after_reduction_in_bytes
        file_stats[basename]['total_shape_size_after_evaluation'] += entry.shape_size_after_evaluation_in_bytes
        file_stats[basename]['total_dwarf_dies'] += entry.dwarf_die_size
        file_stats[basename]['cms_files_loaded'] += entry.cms_files_loaded
        file_stats[basename]['cms_files_cached'] += entry.cms_files_cached

    print("## File-level Statistics")
    print()
    print("Top 20 files by DWARF DIE size:")
    print()
    print("| File | Variables | Before Reduction (Bytes) | After Reduction (Bytes) | After Evaluation (Bytes) | Total DIEs | CMS Loaded | CMS Cache Hits |")
    print("|------|-----------|---------------------------|-------------------------|--------------------------|------------|------------|----------------|")

    # Sort by total DIEs descending
    sorted_files = sorted(file_stats.items(),
                         key=lambda x: x[1]['total_dwarf_dies'], reverse=True)

    for filename, stats in sorted_files[:20]:  # Show top 20 files
        # Extract original source filename from .debug-stats.json
        if filename.endswith('.debug-stats.json'):
            source_filename = filename[:-len('.debug-stats.json')]
        else:
            source_filename = filename

        print("| {} | {:,} | {:,} | {:,} | {:,} | {:,} | {:,} | {:,} |".format(
            source_filename, stats['count'], stats['total_shape_size_before_reduction'],
            stats['total_shape_size_after_reduction'], stats['total_shape_size_after_evaluation'], stats['total_dwarf_dies'], stats['cms_files_loaded'], stats['cms_files_cached']))

    # Add missing/unreadable files section at the end
    if all_missing_files or all_unreadable_files:
        print()
        print("## CMS File Issues")
        print()

        if all_missing_files:
            print("### Files Not Found ({:,} total)".format(len(all_missing_files)))
            print()
            missing_list = ", ".join("`{}`".format(f) for f in sorted(all_missing_files))
            print(missing_list)
            print()

        if all_unreadable_files:
            print("### Files Unreadable ({:,} total)".format(len(all_unreadable_files)))
            print()
            unreadable_list = ", ".join("`{}`".format(f) for f in sorted(all_unreadable_files))
            print(unreadable_list)
            print()

if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Analyze DWARF debug stats JSON files')
    parser.add_argument('directory', nargs='?', default='.',
                       help='Directory to search for *.debug-stats.json files (default: current directory)')
    parser.add_argument('--measure', action='append', metavar='FILE',
                       help='Measure the size of FILE and include in summary. Can be specified multiple times.')

    args = parser.parse_args()

    # Validate that the directory exists
    if not os.path.isdir(args.directory):
        print(f"Error: Directory '{args.directory}' does not exist.", file=sys.stderr)
        sys.exit(1)

    # Measure and display file sizes if requested
    if args.measure:
        file_measurements = measure_file_sizes(args.measure)
        display_file_measurements(file_measurements)

    # Analyze debug stats
    analyze_stats(args.directory)
