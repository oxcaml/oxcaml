#!/usr/bin/env python3
# /// script
# requires-python = ">=3.10"
# dependencies = [
#     "matplotlib",
# ]
# ///
"""
Analyze DWARF debug stats JSON files and produce aggregated statistics.

Usage:
    uv run analyze_debug_stats.py [OPTIONS] [DIRECTORY]

This script searches for all *.debug-stats.json files recursively from the specified
directory (or current directory if not provided) and generates a comprehensive markdown
report of DWARF debugging statistics.

Arguments:
    DIRECTORY              Directory to search for *.debug-stats.json files (default: current directory)

Options:
    --measure FILE         Measure the size of FILE and include in summary. Can be specified multiple times.
                          Also checks for FILE.debug and includes its size if it exists.

Prerequisites:
    - uv package manager
    - Python 3.10 or later (handled by uv)
    - matplotlib library (automatically installed by uv from script dependencies)

Input:
    - Searches for all *.debug-stats.json files in the specified directory and subdirectories
    - Files should be generated by compiling with -ddwarf-shape-reduction-diags flag

Output:
    - Markdown-formatted report printed to stdout
    - Includes histograms (as inline SVG), top values analysis, and file-level statistics
    - Summary includes total files processed, variables analyzed, and DWARF DIEs generated

Example workflow:
    1. Compile OCaml files with debugging flags:
       ocamlopt -ddwarf-shape-reduction-diags -gno-upstream-dwarf -shape-format debugging-shapes -g file.ml

    2. Run analysis:
       uv run analyze_debug_stats.py . > report.md

    3. View the generated markdown report in a markdown renderer:
       mdcat report.md
       # or open in your preferred markdown viewer/browser
"""

import json
import glob
import os
import sys
import argparse
import subprocess
import re
from collections import defaultdict

import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
from io import BytesIO

class StatEntry:
    def __init__(self, type_name, initial_size_memory, reduced_size_memory, evaluated_size_memory,
                 initial_size, reduced_size, evaluated_size, reduction_steps,
                 evaluation_steps, dwarf_die_size, cms_files_loaded, cms_files_cached, 
                 cms_files_missing, cms_files_unreadable, file_path):
        self.type_name = type_name
        self.initial_size_memory = initial_size_memory
        self.reduced_size_memory = reduced_size_memory
        self.evaluated_size_memory = evaluated_size_memory
        self.initial_size = initial_size
        self.reduced_size = reduced_size
        self.evaluated_size = evaluated_size
        self.reduction_steps = reduction_steps
        self.evaluation_steps = evaluation_steps
        self.dwarf_die_size = dwarf_die_size
        self.cms_files_loaded = cms_files_loaded
        self.cms_files_cached = cms_files_cached
        self.cms_files_missing = cms_files_missing
        self.cms_files_unreadable = cms_files_unreadable
        self.file_path = file_path

class FileMeasurement:
    def __init__(self, file_path, main_size, debug_size=None, dwarf_sections={}, total_dwarf_size=0):
        self.file_path = file_path
        self.main_size = main_size
        self.debug_size = debug_size
        self.dwarf_sections = dwarf_sections
        self.total_dwarf_size = total_dwarf_size

def parse_json_file(file_path):
    """Parse a single JSON file and return (entries, file_metadata) tuple."""
    entries = []
    file_metadata = {'compilation_parameters': None}
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)

        # Extract compilation parameters
        file_metadata['compilation_parameters'] = data.get('compilation_parameters', None)

        # Process variables
        for var_data in data.get('variables', []):
            try:
                entry = StatEntry(
                    type_name=var_data['type'],
                    initial_size_memory=int(var_data['initial_size_memory']),
                    reduced_size_memory=int(var_data['reduced_size_memory']),
                    evaluated_size_memory=int(var_data['evaluated_size_memory']),
                    initial_size=int(var_data['initial_size']),
                    reduced_size=int(var_data['reduced_size']),
                    evaluated_size=int(var_data['evaluated_size']),
                    reduction_steps=int(var_data['reduction_steps']),
                    evaluation_steps=int(var_data['evaluation_steps']),
                    dwarf_die_size=int(var_data['dwarf_die_size']),
                    cms_files_loaded=int(var_data['cms_files_loaded']),
                    cms_files_cached=int(var_data['cms_files_cached']),
                    cms_files_missing=var_data.get('cms_files_missing', []),
                    cms_files_unreadable=var_data.get('cms_files_unreadable', []),
                    file_path=file_path
                )
                entries.append(entry)
            except (ValueError, KeyError) as e:
                print("Warning: Skipping malformed variable in {}: {}".format(file_path, e))

    except (json.JSONDecodeError, IOError) as e:
        print("Warning: Could not parse {}: {}".format(file_path, e))

    return entries, file_metadata

def analyze_dwarf_sections(file_path):
    """Analyze DWARF sections in a binary using objdump."""
    try:
        # Run objdump to get section headers
        result = subprocess.run(['objdump', '-h', file_path],
                              capture_output=True, text=True, check=True)

        # Parse the output to find debug sections using regex
        dwarf_sections = {}
        # Match objdump section lines: idx section_name size vma lma file_off algn
        section_pattern = re.compile(r'^\s*\d+\s+(\.debug\w*)\s+([0-9a-fA-F]+)')

        for line in result.stdout.split('\n'):
            match = section_pattern.match(line)
            if match:
                section_name = match.group(1)
                size_hex = match.group(2)
                try:
                    size_bytes = int(size_hex, 16)
                    dwarf_sections[section_name] = size_bytes
                except ValueError:
                    continue

        return dwarf_sections
    except (subprocess.CalledProcessError, FileNotFoundError):
        return {}

def measure_file_sizes(file_paths):
    """Measure the sizes of the specified files and their .debug variants if they exist."""
    measurements = []

    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File '{file_path}' does not exist, skipping measurement.", file=sys.stderr)
            continue

        # Get the main file size
        main_size = os.path.getsize(file_path)

        # Check for .debug variant
        debug_file_path = file_path + ".debug"
        debug_size = None
        if os.path.exists(debug_file_path):
            debug_size = os.path.getsize(debug_file_path)

        # Analyze DWARF sections in the main binary
        dwarf_sections = analyze_dwarf_sections(file_path)

        # Also analyze DWARF sections in the .debug file if it exists
        if debug_size is not None:
            debug_dwarf_sections = analyze_dwarf_sections(debug_file_path)
            # Merge the sections, summing sizes for sections that appear in both
            for section_name, section_size in debug_dwarf_sections.items():
                if section_name in dwarf_sections:
                    dwarf_sections[section_name] += section_size
                else:
                    dwarf_sections[section_name] = section_size

        total_dwarf_size = sum(dwarf_sections.values())

        measurements.append(FileMeasurement(file_path, main_size, debug_size, dwarf_sections, total_dwarf_size))

    return measurements

# Not currently used - kept for potential fallback
def create_histogram_text(values, title, num_buckets=10):
    """Create a text-based histogram with integer buckets."""
    if not values:
        return "{}: No data\n".format(title)

    min_val = min(values)
    max_val = max(values)

    if min_val == max_val:
        return "{}: All values are {}\n".format(title, min_val)

    # Calculate integer bucket size, ensuring we cover the full range
    range_size = max_val - min_val
    bucket_size = max(1, (range_size + num_buckets - 1) // num_buckets)  # Round up division

    buckets = [0] * num_buckets
    bucket_ranges = []

    for i in range(num_buckets):
        start = min_val + i * bucket_size
        end = min_val + (i + 1) * bucket_size
        bucket_ranges.append((start, end))

    # Assign values to buckets
    for val in values:
        bucket_idx = min((val - min_val) // bucket_size, num_buckets - 1)
        buckets[bucket_idx] += 1

    # Create text histogram
    result = "{} (Range: {}-{}):\n".format(title, min_val, max_val)
    max_count = max(buckets)
    scale = 50.0 / max_count if max_count > 0 else 1

    for i, count in enumerate(buckets):
        start, end = bucket_ranges[i]
        bar_length = int(count * scale)
        bar = '#' * bar_length
        # Show inclusive start, exclusive end for integer ranges with comma formatting
        result += "[{:>8,}-{:>8,}): {:>6,} {}\n".format(start, end, count, bar)

    return result + "\n"

def plot_histogram_on_axis(ax, values, bins, title, xlabel='Value', ylabel='Frequency', log_scale=True):
    """Helper function to plot a histogram on a given axis."""
    ax.hist(values, bins=bins, edgecolor='black', alpha=0.7)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.grid(True, alpha=0.3)
    ax.ticklabel_format(style='plain')
    if log_scale:
        ax.set_yscale('log')

def create_dual_histograms(values, title, num_buckets=10):
    """Create side-by-side log scale and focused histograms and return as SVG."""
    min_val = min(values)
    max_val = max(values)

    # Calculate focused histogram data
    range_size = max_val - min_val
    bucket_size = max(1, (range_size + num_buckets - 1) // num_buckets)

    buckets = [0] * num_buckets
    bucket_ranges = []

    for i in range(num_buckets):
        start = min_val + i * bucket_size
        end = min_val + (i + 1) * bucket_size
        bucket_ranges.append((start, end))

    # Assign values to buckets
    for val in values:
        bucket_idx = min((val - min_val) // bucket_size, num_buckets - 1)
        buckets[bucket_idx] += 1

    # Find the bucket with the most entries
    max_bucket_idx = buckets.index(max(buckets))
    focus_start, focus_end = bucket_ranges[max_bucket_idx]

    # Filter values to only those in the largest bucket
    focused_values = [v for v in values if focus_start <= v < focus_end]

    # Always create side-by-side subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))

    # Log scale histogram
    plot_histogram_on_axis(ax1, values, num_buckets, f"{title} (Log Scale)",
                         xlabel=f"Value ({min_val:,}-{max_val:,})")

    # Focused histogram (even if it doesn't make much sense)
    focused_bins = min(10, len(set(focused_values))) if len(focused_values) > 1 else 1
    plot_histogram_on_axis(ax2, focused_values, focused_bins,
                         f"{title} (Focused Log Scale)",
                         xlabel=f"Value ({focus_start:,}-{focus_end:,})")

    # Save to SVG in memory
    buf = BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format='svg', bbox_inches='tight')
    plt.close()

    # Get SVG data and remove newlines/whitespace
    buf.seek(0)
    svg_data = buf.getvalue().decode('utf-8')
    buf.close()

    # Remove newlines and excess whitespace to make it a single line
    svg_data = ' '.join(svg_data.split())

    return svg_data

def create_histogram(values, title, num_buckets=10):
    """Create histogram using matplotlib."""
    if not values:
        return f"{title}: No data\n\n"

    svg_data = create_dual_histograms(values, title, num_buckets)
    return f"\n{svg_data}\n"

def display_file_measurements(file_measurements):
    """Display file size measurements if any are provided."""
    if file_measurements:
        print("# File Measurements")
        print()
        for measurement in file_measurements:
            # Total size is main binary + debug sidecar (they are separate files)
            total_size = measurement.main_size + (measurement.debug_size or 0)
            dwarf_percentage = (measurement.total_dwarf_size / total_size * 100) if total_size > 0 else 0

            print("## `{}`".format(measurement.file_path))
            print("- **Total size**: {:,} bytes".format(total_size))
            if measurement.debug_size is not None:
                print("  - Main binary: {:,} bytes".format(measurement.main_size))
                print("  - Debug sidecar: {:,} bytes".format(measurement.debug_size))
            print("- **DWARF sections** (aggregated): {:,} bytes ({:.1f}% of total)".format(
                measurement.total_dwarf_size, dwarf_percentage))

            if measurement.dwarf_sections:
                print("  - DWARF section breakdown:")
                # Sort sections by size descending
                sorted_sections = sorted(measurement.dwarf_sections.items(),
                                       key=lambda x: x[1], reverse=True)
                for section_name, section_size in sorted_sections:
                    section_percentage = (section_size / measurement.total_dwarf_size * 100) if measurement.total_dwarf_size > 0 else 0
                    print("    - `{}`: {:,} bytes ({:.1f}% of DWARF)".format(
                        section_name, section_size, section_percentage))
            print()

def analyze_stats(search_dir="."):
    """Main analysis function."""
    # Find all JSON files in the specified directory
    search_pattern = os.path.join(search_dir, "**/*.debug-stats.json")
    json_files = glob.glob(search_pattern, recursive=True)

    if not json_files:
        print(f"No .debug-stats.json files found in '{search_dir}' or its subdirectories.")
        return

    # Parse all files
    all_entries = []
    all_file_metadata = []
    all_missing_files = set()
    all_unreadable_files = set()
    file_count = 0
    for json_file in json_files:
        entries, file_metadata = parse_json_file(json_file)
        if entries:
            all_entries.extend(entries)
            all_file_metadata.append((json_file, file_metadata))
            file_count += 1
            # Collect missing/unreadable files from all variables in this file
            for entry in entries:
                all_missing_files.update(entry.cms_files_missing)
                all_unreadable_files.update(entry.cms_files_unreadable)

    if not all_entries:
        print("No valid entries found.")
        return

    # Calculate total DWARF DIEs
    total_dwarf_dies = sum(e.dwarf_die_size for e in all_entries)

    # CMS file statistics for summary - aggregate from per-variable data
    total_cms_loaded = sum(e.cms_files_loaded for e in all_entries)
    total_cms_cached = sum(e.cms_files_cached for e in all_entries)

    # Count files that have variables with CMS files loaded
    files_with_cms = set()
    for entry in all_entries:
        if entry.cms_files_loaded > 0:
            files_with_cms.add(entry.file_path)
    files_with_cms_loaded = len(files_with_cms)

    # Print summary header
    print("# DWARF Debug Statistics Analysis")
    print()
    print("- **Summary:** {:,} files, {:,} variables, {:,} total DIEs".format(file_count, len(all_entries), total_dwarf_dies))
    print("- **CMS Files:** {:,} loaded across {:,} files, {:,} cache hits".format(total_cms_loaded, files_with_cms_loaded, total_cms_cached))

    # Print compilation parameters - fail if not consistent
    compilation_params_sets = set()
    for _, metadata in all_file_metadata:
        params = metadata.get('compilation_parameters')
        if params:
            # Convert to a frozenset of items for hashability
            params_tuple = tuple(sorted(params.items()))
            compilation_params_sets.add(params_tuple)

    if not compilation_params_sets:
        print("- **Error:** No compilation parameters found in JSON files.")
    elif len(compilation_params_sets) > 1:
        print("- **Error:** Files were compiled with different parameters. All files must use the same configuration.")
    else:
        # All files used the same parameters
        params_dict = dict(list(compilation_params_sets)[0])
        config_parts = []
        for key, value in sorted(params_dict.items()):
            # Shorten parameter names for display
            short_name = key.replace('gdwarf_config_', '').replace('_', '-')
            config_parts.append("{}={}".format(short_name, value))
        print("- **Configuration:** {}".format(", ".join(config_parts)))
    print()

    print("## Individual Variable Statistics")
    print()


    # Aggregate statistics for individual variables
    initial_sizes = [e.initial_size for e in all_entries]
    initial_sizes_memory = [e.initial_size_memory for e in all_entries]
    reduced_sizes = [e.reduced_size for e in all_entries]
    reduced_sizes_memory = [e.reduced_size_memory for e in all_entries]
    evaluated_sizes = [e.evaluated_size for e in all_entries]
    evaluated_sizes_memory = [e.evaluated_size_memory for e in all_entries]
    reduction_steps = [e.reduction_steps for e in all_entries]
    evaluation_steps = [e.evaluation_steps for e in all_entries]
    dwarf_die_sizes = [e.dwarf_die_size for e in all_entries]
    cms_files_loaded = [e.cms_files_loaded for e in all_entries]
    cms_files_cached = [e.cms_files_cached for e in all_entries]

    types = [e.type_name for e in all_entries]

    # Top values analysis function
    def print_top_values(values, labels, title, n=5):
        """Print top N values with their labels, deduplicated by (value, label) pairs."""
        if not values:
            return

        # Create a set to deduplicate (value, label) pairs
        unique_pairs = set(zip(values, labels))

        # Sort by value descending
        sorted_pairs = sorted(unique_pairs, key=lambda x: x[0], reverse=True)

        print("#### Top {} {}".format(n, title))
        print()
        for i, (value, label) in enumerate(sorted_pairs[:n]):
            print("{:2d}. **{:,}** - `{}`".format(i+1, value, label))
        print()

    # Initial Sizes section (non-memory first, then memory)
    print("### Initial Sizes")
    print(create_histogram(initial_sizes, "Initial Sizes"), end="")
    print_top_values(initial_sizes, types, "Initial Sizes")

    print("### Initial Sizes (Memory)")
    print(create_histogram(initial_sizes_memory, "Initial Sizes (Memory)"), end="")
    print_top_values(initial_sizes_memory, types, "Initial Sizes (Memory)")

    # Reduced Sizes section (non-memory first, then memory)
    print("### Reduced Sizes")
    print(create_histogram(reduced_sizes, "Reduced Sizes"), end="")
    print_top_values(reduced_sizes, types, "Reduced Sizes")

    print("### Reduced Sizes (Memory)")
    print(create_histogram(reduced_sizes_memory, "Reduced Sizes (Memory)"), end="")
    print_top_values(reduced_sizes_memory, types, "Reduced Sizes (Memory)")

    # Evaluated Sizes section (non-memory first, then memory)
    print("### Evaluated Sizes")
    print(create_histogram(evaluated_sizes, "Evaluated Sizes"), end="")
    print_top_values(evaluated_sizes, types, "Evaluated Sizes")

    print("### Evaluated Sizes (Memory)")
    print(create_histogram(evaluated_sizes_memory, "Evaluated Sizes (Memory)"), end="")
    print_top_values(evaluated_sizes_memory, types, "Evaluated Sizes (Memory)")

    # Reduction Steps section
    print("### Reduction Steps")
    print(create_histogram(reduction_steps, "Reduction Steps"), end="")
    print_top_values(reduction_steps, types, "Reduction Steps")

    # Evaluation Steps section
    print("### Evaluation Steps")
    print(create_histogram(evaluation_steps, "Evaluation Steps"), end="")
    print_top_values(evaluation_steps, types, "Evaluation Steps")

    # DWARF DIE Sizes section
    print("### DWARF DIE Sizes")
    print(create_histogram(dwarf_die_sizes, "DWARF DIE Sizes"), end="")
    print_top_values(dwarf_die_sizes, types, "DWARF DIE Sizes")

    # CMS Files Loaded section
    print("### CMS Files Loaded")
    print(create_histogram(cms_files_loaded, "CMS Files Loaded"), end="")
    print_top_values(cms_files_loaded, types, "CMS Files Loaded")

    # CMS Files Cached section
    print("### CMS Files Cached")
    print(create_histogram(cms_files_cached, "CMS Files Cached"), end="")
    print_top_values(cms_files_cached, types, "CMS Files Cached")

    # File-level aggregation and statistics
    file_stats = defaultdict(lambda: {'count': 0, 'total_initial_memory': 0, 'total_reduced_memory': 0, 'total_initial': 0, 'total_reduced': 0, 'total_dwarf_dies': 0, 'cms_files_loaded': 0, 'cms_files_cached': 0})

    for entry in all_entries:
        basename = os.path.basename(entry.file_path)
        file_stats[basename]['count'] += 1
        file_stats[basename]['total_initial_memory'] += entry.initial_size_memory
        file_stats[basename]['total_reduced_memory'] += entry.reduced_size_memory
        file_stats[basename]['total_initial'] += entry.initial_size
        file_stats[basename]['total_reduced'] += entry.reduced_size
        file_stats[basename]['total_dwarf_dies'] += entry.dwarf_die_size
        file_stats[basename]['cms_files_loaded'] += entry.cms_files_loaded
        file_stats[basename]['cms_files_cached'] += entry.cms_files_cached

    print("## File-level Statistics")
    print()
    print("Top 20 files by DWARF DIE size:")
    print()
    print("| File | Variables | Memory Initial | Memory Reduced | Size Initial | Size Reduced | Total DIEs | CMS Loaded | CMS Cached |")
    print("|------|-----------|----------------|----------------|--------------|--------------|------------|------------|------------|")

    # Sort by total DIEs descending
    sorted_files = sorted(file_stats.items(),
                         key=lambda x: x[1]['total_dwarf_dies'], reverse=True)

    for filename, stats in sorted_files[:20]:  # Show top 20 files
        # Extract original source filename from .debug-stats.json
        if filename.endswith('.debug-stats.json'):
            source_filename = filename[:-len('.debug-stats.json')]
        else:
            source_filename = filename

        print("| {} | {:,} | {:,} | {:,} | {:,} | {:,} | {:,} | {:,} | {:,} |".format(
            source_filename, stats['count'], stats['total_initial_memory'],
            stats['total_reduced_memory'], stats['total_initial'], stats['total_reduced'],
            stats['total_dwarf_dies'], stats['cms_files_loaded'], stats['cms_files_cached']))
    
    # Add missing/unreadable files section at the end
    if all_missing_files or all_unreadable_files:
        print()
        print("## CMS File Issues")
        print()
        
        if all_missing_files:
            print("### Files Not Found ({:,} total)".format(len(all_missing_files)))
            print()
            for missing_file in sorted(all_missing_files):
                print("- `{}`".format(missing_file))
            print()
        
        if all_unreadable_files:
            print("### Files Unreadable ({:,} total)".format(len(all_unreadable_files)))
            print()
            for unreadable_file in sorted(all_unreadable_files):
                print("- `{}`".format(unreadable_file))
            print()

if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Analyze DWARF debug stats JSON files')
    parser.add_argument('directory', nargs='?', default='.',
                       help='Directory to search for *.debug-stats.json files (default: current directory)')
    parser.add_argument('--measure', action='append', metavar='FILE',
                       help='Measure the size of FILE and include in summary. Can be specified multiple times.')

    args = parser.parse_args()

    # Validate that the directory exists
    if not os.path.isdir(args.directory):
        print(f"Error: Directory '{args.directory}' does not exist.", file=sys.stderr)
        sys.exit(1)

    # Measure and display file sizes if requested
    if args.measure:
        file_measurements = measure_file_sizes(args.measure)
        display_file_measurements(file_measurements)

    # Analyze debug stats
    analyze_stats(args.directory)
